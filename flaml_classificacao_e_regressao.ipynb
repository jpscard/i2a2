{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "flaml_classificacao_e_regressao.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpscard/i2a2/blob/main/flaml_classificacao_e_regressao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbA740Dfx_aV"
      },
      "source": [
        "## FLAML: A Fast and Lightweight AutoML Library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Instalação da biblioteca"
      ],
      "metadata": {
        "id": "emP5gF6xB0C3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhC_ulqGvYSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a19793d-e48f-4ba2-a9c4-68c3445023d1"
      },
      "source": [
        "!pip install flaml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flaml\n",
            "  Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: NumPy>=1.17 in /usr/local/lib/python3.10/dist-packages (from flaml) (1.25.2)\n",
            "Installing collected packages: flaml\n",
            "Successfully installed flaml-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BSJhlvY3Db0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa a classe AutoML da biblioteca FLAML.\n",
        "from flaml import AutoML\n",
        "\n",
        " # Importa a função load_iris da biblioteca scikit-learn para carregar o conjunto de dados Iris.\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Importa função load_diabetes da biblioteca scikit-learn para carregar o conjunto de dados diabetes.\n",
        "from sklearn.datasets import load_diabetes"
      ],
      "metadata": {
        "id": "7HAeFNJWDcJD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00xUXNvfwyXV"
      },
      "source": [
        "#1. Problema de Classificação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDEkaxcDvJuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb762dcb-e950-4d0e-81b4-531d497a1869"
      },
      "source": [
        "# Inicializa uma instância do AutoML\n",
        "automl = AutoML()\n",
        "\n",
        "# Especifica o objetivo e a restrição do AutoML\n",
        "automl_settings = {\n",
        "    \"time_budget\": 10,  # tempo em segundos que o AutoML pode gastar na busca do melhor modelo\n",
        "    \"metric\": 'accuracy',  # métrica usada para avaliar os modelos (precisão)\n",
        "    \"task\": 'classification',  # tipo de tarefa (classificação)\n",
        "    \"log_file_name\": \"iris.log\",  # nome do arquivo de log onde as informações do treinamento serão salvas\n",
        "}\n",
        "\n",
        "# Carrega o conjunto de dados Iris e separa em características (X) e rótulos (y)\n",
        "X_train, y_train = load_iris(return_X_y=True)\n",
        "\n",
        "# Treina o modelo com os dados de entrada rotulados\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 05-22 21:14:57] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 05-22 21:14:57] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 05-22 21:14:57] {1789} INFO - Minimizing error metric: 1-accuracy\n",
            "[flaml.automl.logger: 05-22 21:14:57] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 05-22 21:14:57] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:14:57] {2345} INFO - Estimated sufficient time budget=5846s. Estimated necessary time budget=135s.\n",
            "[flaml.automl.logger: 05-22 21:14:57] {2392} INFO -  at 0.6s,\testimator lgbm's best error=0.0733,\tbest estimator lgbm's best error=0.0733\n",
            "[flaml.automl.logger: 05-22 21:14:57] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:14:58] {2392} INFO -  at 1.0s,\testimator lgbm's best error=0.0733,\tbest estimator lgbm's best error=0.0733\n",
            "[flaml.automl.logger: 05-22 21:14:58] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:14:58] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
            "[flaml.automl.logger: 05-22 21:14:58] {2219} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:14:59] {2392} INFO -  at 2.2s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
            "[flaml.automl.logger: 05-22 21:14:59] {2219} INFO - iteration 4, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:14:59] {2392} INFO -  at 2.4s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0533\n",
            "[flaml.automl.logger: 05-22 21:14:59] {2219} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 3.8s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0533\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 3.9s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 3.9s,\testimator lgbm's best error=0.0467,\tbest estimator lgbm's best error=0.0467\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.0s,\testimator lgbm's best error=0.0467,\tbest estimator lgbm's best error=0.0467\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 9, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.1s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 10, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.1s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 11, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 12, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 13, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.3s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.3s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.4s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.5s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 17, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2392} INFO -  at 4.6s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:01] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2392} INFO -  at 4.8s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2219} INFO - iteration 19, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2392} INFO -  at 4.9s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2219} INFO - iteration 20, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2392} INFO -  at 5.0s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2219} INFO - iteration 21, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2392} INFO -  at 5.1s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:02] {2219} INFO - iteration 22, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:03] {2392} INFO -  at 5.9s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:03] {2219} INFO - iteration 23, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:03] {2392} INFO -  at 6.8s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:03] {2219} INFO - iteration 24, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2392} INFO -  at 6.9s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2219} INFO - iteration 25, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2392} INFO -  at 7.1s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2219} INFO - iteration 26, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2392} INFO -  at 7.4s,\testimator extra_tree's best error=0.1533,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2219} INFO - iteration 27, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2392} INFO -  at 7.6s,\testimator extra_tree's best error=0.1533,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:04] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2392} INFO -  at 7.9s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2219} INFO - iteration 29, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2392} INFO -  at 8.0s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2392} INFO -  at 8.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2392} INFO -  at 8.3s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2219} INFO - iteration 32, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2392} INFO -  at 8.7s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:05] {2219} INFO - iteration 33, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2392} INFO -  at 8.9s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2219} INFO - iteration 34, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2392} INFO -  at 9.3s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2219} INFO - iteration 35, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2392} INFO -  at 9.6s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:06] {2219} INFO - iteration 36, current learner rf\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2392} INFO -  at 9.9s,\testimator rf's best error=0.0867,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2219} INFO - iteration 37, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2392} INFO -  at 10.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2628} INFO - retrain lgbm for 0.0s\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2631} INFO - retrained model: LGBMClassifier(colsample_bytree=0.9712608239851066,\n",
            "               learning_rate=0.5066472382072831, max_bin=63,\n",
            "               min_child_samples=9, n_estimators=1, n_jobs=-1, num_leaves=4,\n",
            "               reg_alpha=0.008673969948933881, reg_lambda=0.47452813352268625,\n",
            "               verbose=-1)\n",
            "[flaml.automl.logger: 05-22 21:15:07] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 05-22 21:15:07] {1932} INFO - Time taken to find the best model: 4.080369472503662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Predição da classificação"
      ],
      "metadata": {
        "id": "S6sB1kG9BoZG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-3p9iLJvWwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd770b3-a69d-4df7-e66d-e2538a651984"
      },
      "source": [
        "# Predição\n",
        "print(automl.predict_proba(X_train))\n",
        "\n",
        "# Exibe o melhor modelo\n",
        "print(automl.model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.81927853 0.15011003 0.03061144]\n",
            " [0.7725578  0.20229926 0.02514294]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.94986785 0.0227757  0.02735645]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.81927853 0.15011003 0.03061144]\n",
            " [0.7725578  0.20229926 0.02514294]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.95482095 0.02289446 0.02228459]\n",
            " [0.03532049 0.90765492 0.05702459]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.02979822 0.76574527 0.20445651]\n",
            " [0.02425689 0.94536674 0.03037637]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.04597815 0.88593955 0.0680823 ]\n",
            " [0.03444868 0.93549223 0.03005908]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.03444868 0.93549223 0.03005908]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.03444868 0.93549223 0.03005908]\n",
            " [0.03532049 0.90765492 0.05702459]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.03464616 0.94085506 0.02449878]\n",
            " [0.02409355 0.93900101 0.03690544]\n",
            " [0.02425689 0.94536674 0.03037637]\n",
            " [0.05607781 0.22602748 0.71789471]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02979822 0.76574527 0.20445651]\n",
            " [0.03532049 0.90765492 0.05702459]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.03421511 0.8792491  0.08653579]\n",
            " [0.04750441 0.10416041 0.84833518]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.03464616 0.94085506 0.02449878]\n",
            " [0.02425689 0.94536674 0.03037637]\n",
            " [0.03444868 0.93549223 0.03005908]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.05448352 0.24041868 0.70509779]\n",
            " [0.02426335 0.94561863 0.03011802]\n",
            " [0.03209893 0.93802491 0.02987616]\n",
            " [0.03487663 0.89624882 0.06887455]\n",
            " [0.02425689 0.94536674 0.03037637]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02425689 0.94536674 0.03037637]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.03444868 0.93549223 0.03005908]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.20190513 0.77324896 0.02484591]\n",
            " [0.02439742 0.95084371 0.02475887]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02748767 0.04665313 0.9258592 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.06616033 0.26666613 0.66717354]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02748767 0.04665313 0.9258592 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.04750441 0.10416041 0.84833518]\n",
            " [0.02748767 0.04665313 0.9258592 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.06077965 0.52218884 0.41703151]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.0436898  0.17609634 0.78021386]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.0436898  0.17609634 0.78021386]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.05607781 0.22602748 0.71789471]\n",
            " [0.0436898  0.17609634 0.78021386]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.04765475 0.16284178 0.78950347]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.05448352 0.24041868 0.70509779]\n",
            " [0.04765475 0.16284178 0.78950347]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.05607781 0.22602748 0.71789471]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02748767 0.04665313 0.9258592 ]\n",
            " [0.02748767 0.04665313 0.9258592 ]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.04750441 0.10416041 0.84833518]\n",
            " [0.02201018 0.02892832 0.9490615 ]\n",
            " [0.02913985 0.02871742 0.94214272]\n",
            " [0.02748767 0.04665313 0.9258592 ]]\n",
            "<flaml.automl.model.LGBMEstimator object at 0x7d93ca0d6380>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXE4twtlwk1x"
      },
      "source": [
        "# 2. Regressão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQ6nHCMwEFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dc9305-3885-40af-ebdf-65a27bc3b4e2"
      },
      "source": [
        "# Inicializa uma instância do AutoML\n",
        "automl = AutoML()\n",
        "\n",
        "# Especifica o objetivo e a restrição do AutoML\n",
        "automl_settings = {\n",
        "    \"time_budget\": 10,  # tempo em segundos que o AutoML pode gastar na busca do melhor modelo\n",
        "    \"metric\": 'r2',  # métrica usada para avaliar os modelos (coeficiente de determinação)\n",
        "    \"task\": 'regression',  # tipo de tarefa (regressão)\n",
        "    \"log_file_name\": \"diabetes.log\",  # nome do arquivo de log onde as informações do treinamento serão salvas\n",
        "}\n",
        "\n",
        "# Carrega o conjunto de dados Boston e separa em características (X) e rótulos (y)\n",
        "X_train, y_train = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Treina o modelo com os dados de entrada rotulados\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Realiza previsões usando o conjunto de dados de treinamento\n",
        "print(automl.predict(X_train))\n",
        "\n",
        "# Exibe o melhor modelo encontrado\n",
        "print(automl.model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 05-22 21:15:07] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 05-22 21:15:07] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 05-22 21:15:07] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 05-22 21:15:07] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2345} INFO - Estimated sufficient time budget=2289s. Estimated necessary time budget=16s.\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.7890,\tbest estimator lgbm's best error=0.7890\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.7890,\tbest estimator lgbm's best error=0.7890\n",
            "[flaml.automl.logger: 05-22 21:15:07] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2392} INFO -  at 0.5s,\testimator lgbm's best error=0.6424,\tbest estimator lgbm's best error=0.6424\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2219} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.6212,\tbest estimator lgbm's best error=0.6212\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.6212,\tbest estimator lgbm's best error=0.6212\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2219} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2392} INFO -  at 1.4s,\testimator xgboost's best error=0.7881,\tbest estimator lgbm's best error=0.6212\n",
            "[flaml.automl.logger: 05-22 21:15:08] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2392} INFO -  at 1.5s,\testimator lgbm's best error=0.6212,\tbest estimator lgbm's best error=0.6212\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2392} INFO -  at 1.8s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2392} INFO -  at 1.9s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2392} INFO -  at 2.1s,\testimator xgboost's best error=0.7881,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2392} INFO -  at 2.3s,\testimator xgboost's best error=0.6367,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:09] {2219} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2392} INFO -  at 2.4s,\testimator xgboost's best error=0.6367,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2392} INFO -  at 2.9s,\testimator extra_tree's best error=0.6620,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2392} INFO -  at 3.1s,\testimator xgboost's best error=0.6367,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:10] {2219} INFO - iteration 14, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2392} INFO -  at 3.6s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2219} INFO - iteration 15, current learner rf\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2392} INFO -  at 4.0s,\testimator rf's best error=0.6089,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2219} INFO - iteration 16, current learner rf\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2392} INFO -  at 4.4s,\testimator rf's best error=0.6089,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:11] {2219} INFO - iteration 17, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:12] {2392} INFO -  at 4.7s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:12] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:12] {2392} INFO -  at 5.4s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:13] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:13] {2392} INFO -  at 6.1s,\testimator xgboost's best error=0.6367,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:13] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 05-22 21:15:14] {2392} INFO -  at 6.8s,\testimator rf's best error=0.6050,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:14] {2219} INFO - iteration 21, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:14] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:14] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:15] {2392} INFO -  at 7.7s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:15] {2219} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:15] {2392} INFO -  at 8.0s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:15] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2392} INFO -  at 8.5s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2392} INFO -  at 8.8s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2219} INFO - iteration 26, current learner lgbm\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2392} INFO -  at 8.9s,\testimator lgbm's best error=0.5641,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2392} INFO -  at 9.2s,\testimator xgboost's best error=0.5763,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:16] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 05-22 21:15:17] {2392} INFO -  at 9.6s,\testimator extra_tree's best error=0.5806,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:17] {2219} INFO - iteration 29, current learner xgboost\n",
            "[flaml.automl.logger: 05-22 21:15:17] {2392} INFO -  at 10.0s,\testimator xgboost's best error=0.5763,\tbest estimator lgbm's best error=0.5641\n",
            "[flaml.automl.logger: 05-22 21:15:17] {2628} INFO - retrain lgbm for 0.1s\n",
            "[flaml.automl.logger: 05-22 21:15:17] {2631} INFO - retrained model: LGBMRegressor(learning_rate=0.6945064905423671, max_bin=1023,\n",
            "              min_child_samples=15, n_estimators=1, n_jobs=-1, num_leaves=4,\n",
            "              reg_alpha=0.006955268652669901, reg_lambda=11.838839163780849,\n",
            "              verbose=-1)\n",
            "[flaml.automl.logger: 05-22 21:15:17] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 05-22 21:15:17] {1932} INFO - Time taken to find the best model: 1.7667515277862549\n",
            "[205.34428157  83.12426786 194.82041823 178.97249433 109.53400233\n",
            "  98.97523598  81.30016292 114.07109204 142.87942875 241.16552984\n",
            " 109.22890941 137.28903646  99.28032891 171.81983581  99.28032891\n",
            " 168.66507885 219.79083674 193.42501447 109.53400233  99.28032891\n",
            "  98.96833357  81.30016292  98.97523598 248.42373367 166.4944358\n",
            " 151.04374784  98.97523598 140.07779892 109.53400233 207.00678612\n",
            " 156.24076238  83.12426786 265.56803941  88.71466015  93.37794128\n",
            " 124.65761767 230.35650551 152.86785278 263.20831656 140.47807908\n",
            " 133.61365454 109.22890941  97.45622397 106.95971293 226.12036954\n",
            " 117.94779911 109.53400233 125.38497046  85.92589769 171.81983581\n",
            "  98.96833357 191.51262471  99.28032891 145.02724012 124.23373321\n",
            "  83.12426786 223.3187397   83.12426786 104.89249586 145.43370564\n",
            "  98.97523598 161.25416705  71.04648949 109.53400233 101.76996341\n",
            " 146.09024602 129.95421574 101.76996341 101.76996341  93.37794128\n",
            "  83.12426786 243.62628182 178.4413926   97.45622397 181.60351458\n",
            " 129.95421574 166.06754775 109.22890941 183.89761417 109.22890941\n",
            " 170.48918379  86.8905552   83.12426786 150.64346768  83.12426786\n",
            " 160.61753062  81.30016292 153.19774457  94.88193456  88.71466015\n",
            "  83.12426786 192.30687476 169.84584788  83.12426786  98.97523598\n",
            " 168.44863099 193.42501447 224.69888238  97.45622397 152.59766287\n",
            " 182.05320134  97.12845693 145.67509362 153.44509751 124.23373321\n",
            " 101.76996341 109.22890941 134.48740663 232.84614    115.35173506\n",
            "  85.92589769  99.28032891 155.92876704 207.00678612 275.28609493\n",
            " 163.24998906 232.84614    206.2009783  192.12737792 163.07827199\n",
            "  93.37794128 191.97698297 258.80046352 173.03478007 140.38979426\n",
            " 163.24998906  98.97523598 145.14561928  99.28032891 226.33484259\n",
            " 261.21249454 109.22890941 101.76996341  81.30016292 156.24076238\n",
            " 226.33484259  98.97523598 240.61017893 276.09190274 240.61017893\n",
            " 175.35656366 226.33484259 172.97881829  83.12426786 184.25474947\n",
            " 221.39108886 169.37534127 235.22274572 128.00002055 171.81983581\n",
            " 221.39108886  99.28032891 192.12737792 101.79173806 184.25474947\n",
            " 211.10230727 109.53400233 129.95421574 109.53400233 240.61017893\n",
            "  93.37794128 233.54432547 140.07779892 224.19271869 152.86785278\n",
            "  83.12426786  81.30016292 275.28609493 216.25288624 160.16384641\n",
            "  85.92589769  93.37794128 260.79628554  97.45622397 155.35748728\n",
            "  98.96833357 180.94695595 214.74889296  83.12426786 152.59766287\n",
            " 168.02174293  99.28032891 183.54448705 164.02756254 212.75307094\n",
            " 173.81565782 211.24096134  98.97523598 127.42552326 109.53400233\n",
            " 201.1890534  140.5198845  102.08195874 140.80797086 163.49512918\n",
            " 184.25474947  83.12426786 182.90115114 134.48740663 149.91017299\n",
            " 102.62777237  71.04648949 205.98650525 217.9667318  196.48292279\n",
            " 223.3187397  169.37534127 219.79083674 191.72709776 150.40315936\n",
            " 114.31140036 150.64346768 116.8598545   88.71466015  83.12426786\n",
            " 276.09190274 211.10230727 243.62628182 152.59766287 168.44863099\n",
            "  96.17957112 152.59766287 135.56638269 125.38497046  93.37794128\n",
            " 232.84614     71.04648949  91.53806464 109.53400233  76.63688178\n",
            " 223.47485914 142.03199411 140.47807908 140.38979426 245.62210383\n",
            " 153.41926225 170.17718845  83.12426786 193.4668199  228.33066461\n",
            " 276.09190274 115.35173506  81.30016292  96.17957112 112.15595484\n",
            "  83.12426786 153.19774457 115.13129703 232.04033218 232.04033218\n",
            " 232.04033218 263.20831656 150.64346768 205.88864641 248.42373367\n",
            "  88.71466015 255.99883369  92.81471749 109.76134277  98.99010823\n",
            "  83.12426786  94.3268271  261.21249454  81.30016292 137.28903646\n",
            " 129.95421574  71.04648949 132.10966126 276.09190274  99.28032891\n",
            " 207.44286847 180.94695595 150.64346768 195.42083649 147.64416106\n",
            " 143.53598739 209.91030932  88.71466015 144.83362394 112.33563217\n",
            " 218.00853723 140.47807908  89.04600072  93.37794128 161.01145965\n",
            " 183.89761417  99.28032891 163.45141799  85.92589769 190.79476515\n",
            " 276.09190274 220.16398274  83.12426786 176.46849874  96.17957112\n",
            " 169.37534127  83.12426786  93.37794128 112.33563217 101.76996341\n",
            " 255.99883369  82.24904873 230.04451017 232.35232752 108.02546627\n",
            " 151.04374784  81.30016292 168.93925893  83.12426786 133.61365454\n",
            " 178.14532611 181.45311964 109.22890941 232.84614    150.64346768\n",
            "  88.71466015 178.12939726 151.30359986 169.84584788 204.20515629\n",
            " 192.30687476 245.62210383 248.42373367 209.00260814 175.32776743\n",
            " 228.33066461 176.13357525 221.39108886 140.07779892 100.47232685\n",
            " 166.06754775  96.17957112 273.29027291 193.42501447  81.30016292\n",
            "  83.12426786 257.9946557  182.90115114 125.38497046 142.63207581\n",
            " 131.44546847 191.51262471 180.94695595 162.3012604  158.52629815\n",
            " 101.76996341 197.52053562 155.3992927   88.71466015  81.30016292\n",
            " 233.54432547  81.30016292  87.20255054 175.32776743 216.25288624\n",
            "  81.30016292  97.45622397 211.24096134  83.12426786 196.48292279\n",
            " 181.45311964 121.19179505 265.56803941 140.47807908 143.53598739\n",
            " 186.91371706 245.91687734 243.92105532 176.83176071 140.07779892\n",
            "  83.12426786 205.98650525  87.20255054 109.22890941 125.38497046\n",
            " 183.17134105 192.30687476 155.3992927  165.92315604  83.12426786\n",
            " 172.55493382  97.45622397 214.74889296  86.8905552  135.22328643\n",
            " 150.40315936  88.71466015 134.48740663 171.53532299  71.04648949\n",
            " 263.20831656  93.37794128  94.88193456 109.22890941 232.04033218\n",
            " 199.7485823   81.30016292 173.33194541 223.3187397  232.84614\n",
            " 162.3012604   83.12426786 158.41539559 255.99883369 188.71099487\n",
            " 276.09190274  83.12426786 160.61753062 158.16802439 168.66507885\n",
            " 173.81565782 134.48740663 262.76640957  83.12426786 155.68737906\n",
            " 142.03199411 221.39108886 160.61753062  78.7923273   93.37794128\n",
            " 106.95971293 215.55470078 240.76629836 146.13205145 140.47807908\n",
            " 115.13129703 154.10522969  81.30016292 278.08772476  99.28032891\n",
            " 109.53400233  91.51628998 205.88864641  83.12426786  83.12426786\n",
            "  93.37794128  83.12426786 232.84614     90.54999401  96.17957112\n",
            " 223.3187397   78.7923273 ]\n",
            "<flaml.automl.model.LGBMEstimator object at 0x7d941cfeffd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThYM5e7TxriP"
      },
      "source": [
        "## 2.1 Predição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5eRkuWMxUcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ddff16d-1e8b-43bf-9d8a-9c234c676a83"
      },
      "source": [
        "# Predição\n",
        "print(automl.predict(X_train))\n",
        "\n",
        "# Exibe o melhor modelo\n",
        "print(automl.model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[205.34428157  83.12426786 194.82041823 178.97249433 109.53400233\n",
            "  98.97523598  81.30016292 114.07109204 142.87942875 241.16552984\n",
            " 109.22890941 137.28903646  99.28032891 171.81983581  99.28032891\n",
            " 168.66507885 219.79083674 193.42501447 109.53400233  99.28032891\n",
            "  98.96833357  81.30016292  98.97523598 248.42373367 166.4944358\n",
            " 151.04374784  98.97523598 140.07779892 109.53400233 207.00678612\n",
            " 156.24076238  83.12426786 265.56803941  88.71466015  93.37794128\n",
            " 124.65761767 230.35650551 152.86785278 263.20831656 140.47807908\n",
            " 133.61365454 109.22890941  97.45622397 106.95971293 226.12036954\n",
            " 117.94779911 109.53400233 125.38497046  85.92589769 171.81983581\n",
            "  98.96833357 191.51262471  99.28032891 145.02724012 124.23373321\n",
            "  83.12426786 223.3187397   83.12426786 104.89249586 145.43370564\n",
            "  98.97523598 161.25416705  71.04648949 109.53400233 101.76996341\n",
            " 146.09024602 129.95421574 101.76996341 101.76996341  93.37794128\n",
            "  83.12426786 243.62628182 178.4413926   97.45622397 181.60351458\n",
            " 129.95421574 166.06754775 109.22890941 183.89761417 109.22890941\n",
            " 170.48918379  86.8905552   83.12426786 150.64346768  83.12426786\n",
            " 160.61753062  81.30016292 153.19774457  94.88193456  88.71466015\n",
            "  83.12426786 192.30687476 169.84584788  83.12426786  98.97523598\n",
            " 168.44863099 193.42501447 224.69888238  97.45622397 152.59766287\n",
            " 182.05320134  97.12845693 145.67509362 153.44509751 124.23373321\n",
            " 101.76996341 109.22890941 134.48740663 232.84614    115.35173506\n",
            "  85.92589769  99.28032891 155.92876704 207.00678612 275.28609493\n",
            " 163.24998906 232.84614    206.2009783  192.12737792 163.07827199\n",
            "  93.37794128 191.97698297 258.80046352 173.03478007 140.38979426\n",
            " 163.24998906  98.97523598 145.14561928  99.28032891 226.33484259\n",
            " 261.21249454 109.22890941 101.76996341  81.30016292 156.24076238\n",
            " 226.33484259  98.97523598 240.61017893 276.09190274 240.61017893\n",
            " 175.35656366 226.33484259 172.97881829  83.12426786 184.25474947\n",
            " 221.39108886 169.37534127 235.22274572 128.00002055 171.81983581\n",
            " 221.39108886  99.28032891 192.12737792 101.79173806 184.25474947\n",
            " 211.10230727 109.53400233 129.95421574 109.53400233 240.61017893\n",
            "  93.37794128 233.54432547 140.07779892 224.19271869 152.86785278\n",
            "  83.12426786  81.30016292 275.28609493 216.25288624 160.16384641\n",
            "  85.92589769  93.37794128 260.79628554  97.45622397 155.35748728\n",
            "  98.96833357 180.94695595 214.74889296  83.12426786 152.59766287\n",
            " 168.02174293  99.28032891 183.54448705 164.02756254 212.75307094\n",
            " 173.81565782 211.24096134  98.97523598 127.42552326 109.53400233\n",
            " 201.1890534  140.5198845  102.08195874 140.80797086 163.49512918\n",
            " 184.25474947  83.12426786 182.90115114 134.48740663 149.91017299\n",
            " 102.62777237  71.04648949 205.98650525 217.9667318  196.48292279\n",
            " 223.3187397  169.37534127 219.79083674 191.72709776 150.40315936\n",
            " 114.31140036 150.64346768 116.8598545   88.71466015  83.12426786\n",
            " 276.09190274 211.10230727 243.62628182 152.59766287 168.44863099\n",
            "  96.17957112 152.59766287 135.56638269 125.38497046  93.37794128\n",
            " 232.84614     71.04648949  91.53806464 109.53400233  76.63688178\n",
            " 223.47485914 142.03199411 140.47807908 140.38979426 245.62210383\n",
            " 153.41926225 170.17718845  83.12426786 193.4668199  228.33066461\n",
            " 276.09190274 115.35173506  81.30016292  96.17957112 112.15595484\n",
            "  83.12426786 153.19774457 115.13129703 232.04033218 232.04033218\n",
            " 232.04033218 263.20831656 150.64346768 205.88864641 248.42373367\n",
            "  88.71466015 255.99883369  92.81471749 109.76134277  98.99010823\n",
            "  83.12426786  94.3268271  261.21249454  81.30016292 137.28903646\n",
            " 129.95421574  71.04648949 132.10966126 276.09190274  99.28032891\n",
            " 207.44286847 180.94695595 150.64346768 195.42083649 147.64416106\n",
            " 143.53598739 209.91030932  88.71466015 144.83362394 112.33563217\n",
            " 218.00853723 140.47807908  89.04600072  93.37794128 161.01145965\n",
            " 183.89761417  99.28032891 163.45141799  85.92589769 190.79476515\n",
            " 276.09190274 220.16398274  83.12426786 176.46849874  96.17957112\n",
            " 169.37534127  83.12426786  93.37794128 112.33563217 101.76996341\n",
            " 255.99883369  82.24904873 230.04451017 232.35232752 108.02546627\n",
            " 151.04374784  81.30016292 168.93925893  83.12426786 133.61365454\n",
            " 178.14532611 181.45311964 109.22890941 232.84614    150.64346768\n",
            "  88.71466015 178.12939726 151.30359986 169.84584788 204.20515629\n",
            " 192.30687476 245.62210383 248.42373367 209.00260814 175.32776743\n",
            " 228.33066461 176.13357525 221.39108886 140.07779892 100.47232685\n",
            " 166.06754775  96.17957112 273.29027291 193.42501447  81.30016292\n",
            "  83.12426786 257.9946557  182.90115114 125.38497046 142.63207581\n",
            " 131.44546847 191.51262471 180.94695595 162.3012604  158.52629815\n",
            " 101.76996341 197.52053562 155.3992927   88.71466015  81.30016292\n",
            " 233.54432547  81.30016292  87.20255054 175.32776743 216.25288624\n",
            "  81.30016292  97.45622397 211.24096134  83.12426786 196.48292279\n",
            " 181.45311964 121.19179505 265.56803941 140.47807908 143.53598739\n",
            " 186.91371706 245.91687734 243.92105532 176.83176071 140.07779892\n",
            "  83.12426786 205.98650525  87.20255054 109.22890941 125.38497046\n",
            " 183.17134105 192.30687476 155.3992927  165.92315604  83.12426786\n",
            " 172.55493382  97.45622397 214.74889296  86.8905552  135.22328643\n",
            " 150.40315936  88.71466015 134.48740663 171.53532299  71.04648949\n",
            " 263.20831656  93.37794128  94.88193456 109.22890941 232.04033218\n",
            " 199.7485823   81.30016292 173.33194541 223.3187397  232.84614\n",
            " 162.3012604   83.12426786 158.41539559 255.99883369 188.71099487\n",
            " 276.09190274  83.12426786 160.61753062 158.16802439 168.66507885\n",
            " 173.81565782 134.48740663 262.76640957  83.12426786 155.68737906\n",
            " 142.03199411 221.39108886 160.61753062  78.7923273   93.37794128\n",
            " 106.95971293 215.55470078 240.76629836 146.13205145 140.47807908\n",
            " 115.13129703 154.10522969  81.30016292 278.08772476  99.28032891\n",
            " 109.53400233  91.51628998 205.88864641  83.12426786  83.12426786\n",
            "  93.37794128  83.12426786 232.84614     90.54999401  96.17957112\n",
            " 223.3187397   78.7923273 ]\n",
            "<flaml.automl.model.LGBMEstimator object at 0x7d941cfeffd0>\n"
          ]
        }
      ]
    }
  ]
}